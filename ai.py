# -*- coding: utf-8 -*-
"""AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_fRrxlzcQ7Juk5wQgb9bYoY9b_6wYc8x

## Preprocessing
"""

from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('https://raw.githubusercontent.com/Om03/Devops/main/neo_v2.csv', index_col='id')
df.boxplot()
df.drop(columns=['name'], inplace=True)
df.isna().sum()
plt.scatter(df.est_diameter_max, df.absolute_magnitude)

"""#standardization with without"""

import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU
from tensorflow.keras.models import Sequential
from yahoo_fin import stock_info as si
import numpy as np
import math
model = Sequential([
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True, input_shape=(5, 1)),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=False),
    Dropout(0.20),
    Dense(5)
])
model_un = Sequential([
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True, input_shape=(5, 1)),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=True),
    LSTM(50, activation=LeakyReLU(0.1), return_sequences=False),
    Dropout(0.20),
    Dense(5)
])
data = si.get_data('MSFT')
data = data.resample('1W').mean()
plt.plot(data['close'])
data = data['close']
mean = np.mean(data.values)
variance = np.var(data.values)
sd = math.sqrt(variance)
data = pd.DataFrame(data)
data['scaled'] = (data['close'] - mean)/sd
plt.plot(data['scaled'])
train_scaled = data[:math.floor(0.8*len(data))]['scaled']
test_scaled = data[math.floor(0.8*len(data)):]['scaled']
train_close = data[:math.floor(0.8*len(data))]['close']
test_close = data[math.floor(0.8*len(data)):]['close']
def batch_gen(df_x):
  X = []
  Y = []
  for i in range(5,len(df_x)-5):
    X.append(df_x[i-5:i])
    Y.append(df_x[i:i+5])

  return np.reshape(np.array(X), (len(X), 5, 1)), np.array(Y)
train_X_close, train_Y_close = batch_gen(train_close)
test_X_close, test_Y_close = batch_gen(test_close)
train_X_scaled, train_Y_scaled = batch_gen(train_scaled)
test_X_scaled, test_Y_scaled = batch_gen(test_scaled)
model.compile(loss= tf.keras.losses.mse, optimizer=tf.keras.optimizers.Adam(), metrics=['MAE'])
model_un.compile(loss= tf.keras.losses.mse, optimizer=tf.keras.optimizers.Adam(), metrics=['MAE'])
history = model.fit(train_X_close, train_Y_close, epochs=50)
history_un = model_un.fit(train_X_scaled, train_Y_scaled, epochs=50)

"""# timeseries 3 que"""

from yahoo_fin import stock_info as si
from statsmodels.tsa.seasonal import seasonal_decompose as s_d

data2 = si.get_data('MSFT')
data2 = data2.resample('1W').mean()
result = s_d(pd.DataFrame(data2['close']))
result.plot()

"""#IMAGE"""

from PIL import Image
import cv2
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
!wget https://static.remove.bg/remove-bg-web/221525818b4ba04e9088d39cdcbd0c7bcdfb052e/assets/start_remove-c851bdf8d3127a24e2d137a55b1b427378cd17385b01aec6e59d5d4b5f39d2ec.png
im = cv2.imread('start_remove-c851bdf8d3127a24e2d137a55b1b427378cd17385b01aec6e59d5d4b5f39d2ec.png')
im = im[-580:, -1000:]
im_1 = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
cv2_imshow(im_1)
im_2 = cv2.resize(im_1, (300,300))
cv2_imshow(im_2)
im_3 = cv2.rotate(im_2, cv2.ROTATE_90_CLOCKWISE)
cv2_imshow(im_3)

"""#graph"""

import networkx as nx
G = nx.Graph(day="Stackoverflow")
df_links = pd.read_csv('https://raw.githubusercontent.com/Om03/Devops/main/stack_network_links.csv')
df_nodes = pd.read_csv('https://raw.githubusercontent.com/Om03/Devops/main/stack_network_nodes.csv')
for index, row in df_nodes.iterrows():
  G.add_node(row['name'], group=row['group'], nodesize=row['nodesize'])
for index, row in df_links.iterrows():
  G.add_weighted_edges_from([(row['source'], row['target'], row['value'])])
color_map = {1:'#f09494', 2:'#eebcbc', 3:'#72bbd0', 4:'#91f0a1', 5:'#629fff', 6:'#bcc2f2',  
             7:'#eebcbc', 8:'#f1f0c0', 9:'#d2ffe7', 10:'#caf3a6', 11:'#ffdf55', 12:'#ef77aa', 
             13:'#d6dcff', 14:'#d2f5f0'}
plt.figure(figsize=(25,25))
options = {
    'edge_color': '#00008b',
    'width': 1,
    'with_labels': True,
    'font_weight': 'regular',
}
colors = [color_map[G.nodes[node]['group']] for node in G]
sizes = [G.nodes[node]['nodesize']*10 for node in G]

nx.draw(G, node_color=colors, node_size=sizes, pos=nx.spring_layout(G, k=0.7, iterations=50), **options)
ax = plt.gca()
ax.collections[0].set_edgecolor("#555555") 
plt.show()
page_rank = nx.pagerank(G)

"""#spatial"""

import imdlib as imd
start_yr = 2021
end_yr = 2021
variable = 'tmin' 
file_dir = (r'/content') 
imd.get_data(variable, start_yr, end_yr,  file_dir=file_dir)
data = imd.open_data(variable, start_yr, end_yr, file_dir)
ds = data.get_xarray()
ds = ds.where(ds['tmin'] != -999.)
ds['tmin'].mean('time').plot()

"""#matplotlib"""

import matplotlib.pyplot as plt
import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/Om03/Devops/main/diabetes.csv')
X = list(df.iloc[:, 0])
Y = list(df.iloc[:, 3])
plt.bar(X, Y, color='g')
plt.title("Diabetes dataset")
plt.show()

import matplotlib.pyplot as plt
x = [1,2,3,4,5,6,7,8,9]
y = [12,34,12,33,56,21,45,56,78]
plt.plot(x, y)

import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/Om03/Devops/main/diabetes.csv')
boxplot = df.boxplot(figsize = (5,5), rot = 90, fontsize= '8', grid = False)

df.head(5)
fig, ax = plt.subplots(figsize=(10,8))
plt.hist(df['Insulin'],bins=5)

"""#Question7"""

import seaborn as sns

sns.set_theme(style="darkgrid")
iris = sns.load_dataset("iris")

# Set up the figure
f, ax = plt.subplots(figsize=(8, 8))
ax.set_aspect("equal")

# Draw a contour plot to represent each bivariate density
sns.kdeplot(
    data=iris.query("species != 'versicolor'"),
    x="sepal_width",
    y="sepal_length",
    hue="species",
    thresh=.1,
)

import numpy as np
import seaborn as sns
sns.set_theme(style="ticks")

rs = np.random.RandomState(42)
x = rs.gamma(2, size=1000)
y = -.5 * x + rs.normal(size=1000)

sns.jointplot(x=x, y=y, kind="hex", color="red")

"""#Fuzzy operaions"""

A = dict()
B = dict()
Union = dict()
Intersection = dict()
Complement = dict()
Difference = dict()
AlSum = dict()
AlDiff = dict()
BoundSum = dict ()
BoundDiff = dict()

A = {"a": 0.2, "b": 0.3, "c": 0.6, "d": 0.6}
B = {"a": 0.9, "b": 0.9, "c": 0.4, "d": 0.5}
 
print('The First Fuzzy Set is :', A)
print('The Second Fuzzy Set is :', B)

for A_key, B_key in zip(A, B):
    A_value = A[A_key]
    B_value = B[B_key]

    Union[A_key] = max(A_value, B_value)
    Intersection[A_key] = min(A_value, B_value)
    Complement [A_key]= 1-A[A_key]
    B_value = 1 - B_value
    Difference[A_key] = min(A_value, B_value)
    AlSum[A_key] = A_value + B_value - (A_value * B_value)
    AlDiff[A_key] = (A_value * B_value)
    BoundSum[A_key] = min(1, (A_value + B_value))
    BoundDiff[A_key] = max(0, (A_value - B_value))

print('Fuzzy Set Union is :', Union)
print('Fuzzy Set Intersection is :', Intersection)
print('Fuzzy Set Complement is :', Complement)
print('Fuzzy Set Difference is :', Difference)
print('Fuzzy Set Algebraic Sum is :', AlSum)
print('Fuzzy Set Algebraic Difference is :', AlDiff)
print('Fuzzy Set Bounded Sum is :', BoundSum)
print('Fuzzy Set Bounded Difference is :', BoundDiff)

